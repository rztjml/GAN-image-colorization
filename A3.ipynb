{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfOXmvzADfpr"
      },
      "source": [
        "# Assignment 3: Image Colourization [50 Pt]\n",
        "\n",
        "In this assignment, you will build models to perform image colourization. That is, given a greyscale image, we wish to predict the colour at each pixel. Image colourization is a difficult problem for many reasons, one of which being that it is ill-posed: for a single greyscale image, there can be multiple, equally valid colourings.\n",
        "\n",
        "To keep the training time manageable we will use the CIFAR-10 data set, which consists of images of size 32x32 pixels. For most of the questions we will use a subset of the dataset. The data loading script is included with the notebooks, and should download automatically the first time it is loaded.\n",
        "\n",
        "We will be starting with a convolutional autoencoder and tweaking it along the way to improve our perforamnce. Then as a second part of the assignment we will compare the autoencoder approach to conditional generative adversarial networks (cGANs).\n",
        "\n",
        "In the process, you are expected to learn to:\n",
        "\n",
        "1. Clean and process the dataset and create greyscale images.\n",
        "2. Implement and modify an autoencoder architecture.\n",
        "3. Tune the hyperparameters of an autoencoder.\n",
        "4. Implement skip connections and other techniques to improve performance.\n",
        "5. Implement a cGAN and compare with an autoencoder.\n",
        "6. Improve on the cGAN by trying one of several techniques to enhance training.\n",
        "\n",
        "*This assignment is based on an assignment developed by Prof. Lisa Zhang and Prof. Jimmy Ba.*\n",
        "\n",
        "### What to submit\n",
        "\n",
        "Submit an HTML file containing all your code, outputs, and write-up\n",
        "from parts A and B. Please take extra effort to make your answers and submissions readable. Do not display unnecessary outputs, only the ones that are important for supporting your answers.\n",
        "\n",
        "You can produce a HTML file directly from Google Colab. The Colab instructions are provided at the end of this document.\n",
        "\n",
        "**Do not submit any other files produced by your code.**\n",
        "\n",
        "Include a link to your colab file in your submission.\n",
        "\n",
        "Please use Google Colab to complete this assignment. If you want to use Jupyter Notebook, please complete the assignment and upload your Jupyter Notebook file to Google Colab for submission."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbnrp2ig1pps"
      },
      "source": [
        "## Colab Link\n",
        "\n",
        "Include a link to your Colab file here. If you would like the TA to look at your\n",
        "Colab file in case your solutions are cut off, **please make sure that your Colab\n",
        "file is publicly accessible at the time of submission**."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "http://"
      ],
      "metadata": {
        "id": "Qgfuc_PO0_nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFMdtipUPNdu"
      },
      "source": [
        "# PART A - Autoencoder\n",
        "In this part we will construct and compare different autoencoder models for the image colourization task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BIpGwANoQOg"
      },
      "source": [
        "#### Helper code\n",
        "\n",
        "Provided are some helper functions for loading and preparing the data. Note that you will need to use the Colab GPU for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTF1TQObE6DG"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Colourization of CIFAR-10 Horses via classification.\n",
        "\"\"\"\n",
        "import argparse\n",
        "import math\n",
        "import time\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "import scipy.misc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wE_dtMgIh0SJ"
      },
      "outputs": [],
      "source": [
        "######################################################################\n",
        "# Setup working directory\n",
        "######################################################################\n",
        "%mkdir -p /content/a3/\n",
        "%cd /content/a3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piDmAsqFG0gU"
      },
      "outputs": [],
      "source": [
        "######################################################################\n",
        "# Helper functions for loading data\n",
        "######################################################################\n",
        "# adapted from\n",
        "# https://github.com/fchollet/keras/blob/master/keras/datasets/cifar10.py\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "import tarfile\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "\n",
        "\n",
        "def get_file(fname, origin, untar=False, extract=False, archive_format=\"auto\", cache_dir=\"data\"):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + \".tar.gz\"\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "\n",
        "    print(\"File path: %s\" % fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print(\"Downloading data from\", origin)\n",
        "\n",
        "        error_msg = \"URL fetch failure on {}: {} -- {}\"\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print(\"Extracting file.\")\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "\n",
        "def load_batch(fpath, label_key=\"labels\"):\n",
        "    \"\"\"Internal utility for parsing CIFAR data.\n",
        "    # Arguments\n",
        "        fpath: path the file to parse.\n",
        "        label_key: key for label data in the retrieve\n",
        "            dictionary.\n",
        "    # Returns\n",
        "        A tuple `(data, labels)`.\n",
        "    \"\"\"\n",
        "    f = open(fpath, \"rb\")\n",
        "    if sys.version_info < (3,):\n",
        "        d = pickle.load(f)\n",
        "    else:\n",
        "        d = pickle.load(f, encoding=\"bytes\")\n",
        "        # decode utf8\n",
        "        d_decoded = {}\n",
        "        for k, v in d.items():\n",
        "            d_decoded[k.decode(\"utf8\")] = v\n",
        "        d = d_decoded\n",
        "    f.close()\n",
        "    data = d[\"data\"]\n",
        "    labels = d[label_key]\n",
        "\n",
        "    data = data.reshape(data.shape[0], 3, 32, 32)\n",
        "    return data, labels\n",
        "\n",
        "\n",
        "def load_cifar10(transpose=False):\n",
        "    \"\"\"Loads CIFAR10 dataset.\n",
        "    # Returns\n",
        "        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
        "    \"\"\"\n",
        "    dirname = \"cifar-10-batches-py\"\n",
        "    origin = \"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "    path = get_file(dirname, origin=origin, untar=True)\n",
        "\n",
        "    num_train_samples = 50000\n",
        "\n",
        "    x_train = np.zeros((num_train_samples, 3, 32, 32), dtype=\"uint8\")\n",
        "    y_train = np.zeros((num_train_samples,), dtype=\"uint8\")\n",
        "\n",
        "    for i in range(1, 6):\n",
        "        fpath = os.path.join(path, \"data_batch_\" + str(i))\n",
        "        data, labels = load_batch(fpath)\n",
        "        x_train[(i - 1) * 10000 : i * 10000, :, :, :] = data\n",
        "        y_train[(i - 1) * 10000 : i * 10000] = labels\n",
        "\n",
        "    fpath = os.path.join(path, \"test_batch\")\n",
        "    x_test, y_test = load_batch(fpath)\n",
        "\n",
        "    y_train = np.reshape(y_train, (len(y_train), 1))\n",
        "    y_test = np.reshape(y_test, (len(y_test), 1))\n",
        "\n",
        "    if transpose:\n",
        "        x_train = x_train.transpose(0, 2, 3, 1)\n",
        "        x_test = x_test.transpose(0, 2, 3, 1)\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7fti3cryStt"
      },
      "outputs": [],
      "source": [
        "# Download CIFAR dataset\n",
        "m = load_cifar10()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2TWonhyn0FK"
      },
      "source": [
        "## Part 1 Data Preparation [4 pt]\n",
        "\n",
        "To start off run the above code to load the CIFAR dataset and then work through the following questions/tasks.\n",
        "\n",
        "### Part (i) [2pt EXPLORATORY]\n",
        "Verify that the dataset has loaded correctly. How many samples do we have? How is the data organized?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6bV7uvsh9Sj"
      },
      "outputs": [],
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "M1edgG7845pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDqVs87xpA40"
      },
      "source": [
        "### Part (ii) [2pt EXPLORATORY]\n",
        "\n",
        "Provided below is sample code to preproces the data to select only images of horses, which will help to simplify the goals of the assignment. The function also converts the colour images to greyscale to create our input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-900ROTMlSPd"
      },
      "outputs": [],
      "source": [
        "# select a single category.\n",
        "HORSE_CATEGORY = 7\n",
        "\n",
        "# convert colour images into greyscale\n",
        "def process(xs, ys, max_pixel=256.0, downsize_input=False):\n",
        "    \"\"\"\n",
        "    Pre-process CIFAR10 images by taking only the horse category,\n",
        "    shuffling, and have colour values be bound between 0 and 1\n",
        "\n",
        "    Args:\n",
        "      xs: the colour RGB pixel values\n",
        "      ys: the category labels\n",
        "      max_pixel: maximum pixel value in the original data\n",
        "    Returns:\n",
        "      xs: value normalized and shuffled colour images\n",
        "      grey: greyscale images, also normalized so values are between 0 and 1\n",
        "    \"\"\"\n",
        "    xs = xs / max_pixel\n",
        "    xs = xs[np.where(ys == HORSE_CATEGORY)[0], :, :, :]\n",
        "    npr.shuffle(xs)\n",
        "\n",
        "    grey = np.mean(xs, axis=1, keepdims=True)\n",
        "\n",
        "    if downsize_input:\n",
        "        downsize_module = nn.Sequential(\n",
        "            nn.AvgPool2d(2),\n",
        "            nn.AvgPool2d(2),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "        )\n",
        "        xs_downsized = downsize_module.forward(torch.from_numpy(xs).float())\n",
        "        xs_downsized = xs_downsized.data.numpy()\n",
        "        return (xs, xs_downsized)\n",
        "    else:\n",
        "        return (xs, grey)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHSeX3ADsQu5"
      },
      "source": [
        "The provided get_batch function creates a dataloader (or function) to batch the samples."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloader for batching samples\n",
        "\n",
        "def get_batch(x, y, batch_size):\n",
        "    \"\"\"\n",
        "    Generated that yields batches of data\n",
        "\n",
        "    Args:\n",
        "      x: input values\n",
        "      y: output values\n",
        "      batch_size: size of each batch\n",
        "    Yields:\n",
        "      batch_x: a batch of inputs of size at most batch_size\n",
        "      batch_y: a batch of outputs of size at most batch_size\n",
        "    \"\"\"\n",
        "    N = np.shape(x)[0]\n",
        "    assert N == np.shape(y)[0]\n",
        "    for i in range(0, N, batch_size):\n",
        "        batch_x = x[i : i + batch_size, :, :, :]\n",
        "        batch_y = y[i : i + batch_size, :, :, :]\n",
        "        yield (batch_x, batch_y)\n"
      ],
      "metadata": {
        "id": "0fNm3w_15C_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjPTh0sjsgkV"
      },
      "source": [
        "Run the above helper code and call the appropriate fucntion to verify and visualize that we are able to generate different batches of data with the correct class of images (i.e., horses).\n",
        "\n",
        "Write code to visualize 5 train and test (validation) images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7zubylhiipE"
      },
      "outputs": [],
      "source": [
        "# code to load different batches of horse dataset\n",
        "\n",
        "print(\"Loading data...\")\n",
        "(x_train, y_train), (x_test, y_test) = load_cifar10()\n",
        "\n",
        "print(\"Transforming data...\")\n",
        "train_rgb, train_grey = process(x_train, y_train)\n",
        "test_rgb, test_grey = process(x_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wek0NfrHjNWE"
      },
      "outputs": [],
      "source": [
        "# shape of data and labels before selection\n",
        "print(x_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Qpj_1-yjfM-"
      },
      "outputs": [],
      "source": [
        "# shape of training data\n",
        "print('Training Data: ', train_rgb.shape, train_grey.shape)\n",
        "# shape of testing data\n",
        "print('Testing Data: ', test_rgb.shape, test_grey.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHBOzMJPpNbq"
      },
      "source": [
        "Load Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfGh9pZOpP3r"
      },
      "outputs": [],
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUg_-oB_o9Wf"
      },
      "source": [
        "Write code to visualize 5 train/test images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bGPAI4Go_q_"
      },
      "outputs": [],
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAqGXV0iK1G9"
      },
      "source": [
        "## Part 2 Image Colourization as Regression [4 pt]\n",
        "\n",
        "There are many ways to frame the problem of image colourization as a machine learning problem. One naive approach is to frame it as a regression problem, where we build a model to predict the RGB intensities at each pixel given the greyscale input. In this case, the outputs are continuous, and so squared error can be used to train the model.\n",
        "\n",
        "In this section, you will be the training neural networks using cloud GPUs. Run the helper code and answer the questions that follow.\n",
        "\n",
        "#### Helper Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hNqfK1AktAE"
      },
      "source": [
        "Regression Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOG_sFloK9gs"
      },
      "outputs": [],
      "source": [
        "class RegressionCNN(nn.Module):\n",
        "    def __init__(self, kernel, num_filters):\n",
        "        # first call parent's initialization function\n",
        "        super().__init__()\n",
        "        padding = kernel // 2\n",
        "\n",
        "        self.downconv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, num_filters, kernel_size=kernel, padding=padding),\n",
        "            nn.BatchNorm2d(num_filters),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),)\n",
        "        self.downconv2 = nn.Sequential(\n",
        "            nn.Conv2d(num_filters, num_filters*2, kernel_size=kernel, padding=padding),\n",
        "            nn.BatchNorm2d(num_filters*2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),)\n",
        "\n",
        "        self.rfconv = nn.Sequential(\n",
        "            nn.Conv2d(num_filters*2, num_filters*2, kernel_size=kernel, padding=padding),\n",
        "            nn.BatchNorm2d(num_filters*2),\n",
        "            nn.ReLU())\n",
        "\n",
        "        self.upconv1 = nn.Sequential(\n",
        "            nn.Conv2d(num_filters*2, num_filters, kernel_size=kernel, padding=padding),\n",
        "            nn.BatchNorm2d(num_filters),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2),)\n",
        "        self.upconv2 = nn.Sequential(\n",
        "            nn.Conv2d(num_filters, 3, kernel_size=kernel, padding=padding),\n",
        "            nn.BatchNorm2d(3),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2),)\n",
        "        self.finalconv = nn.Conv2d(3, 3, kernel_size=kernel, padding=padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.downconv1(x)\n",
        "        out = self.downconv2(out)\n",
        "        out = self.rfconv(out)\n",
        "        out = self.upconv1(out)\n",
        "        out = self.upconv2(out)\n",
        "        out = self.finalconv(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nXIHUDdkbe_"
      },
      "source": [
        "Training code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPJG13lnka2w"
      },
      "outputs": [],
      "source": [
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "def get_torch_vars(xs, ys, gpu=False):\n",
        "    \"\"\"\n",
        "    Helper function to convert numpy arrays to pytorch tensors.\n",
        "    If GPU is used, move the tensors to GPU.\n",
        "\n",
        "    Args:\n",
        "      xs (float numpy tenosor): greyscale input\n",
        "      ys (int numpy tenosor): rgb as labels\n",
        "      gpu (bool): whether to move pytorch tensor to GPU\n",
        "    Returns:\n",
        "      Variable(xs), Variable(ys)\n",
        "    \"\"\"\n",
        "    xs = torch.from_numpy(xs).float()\n",
        "    ys = torch.from_numpy(ys).float()\n",
        "    if gpu:\n",
        "        xs = xs.cuda()\n",
        "        ys = ys.cuda()\n",
        "    return Variable(xs), Variable(ys)\n",
        "\n",
        "def train(args, gen=None):\n",
        "\n",
        "    # Numpy random seed\n",
        "    npr.seed(args.seed)\n",
        "\n",
        "    # Save directory\n",
        "    save_dir = \"outputs/\" + args.experiment_name\n",
        "\n",
        "    # LOAD THE MODEL\n",
        "    if gen is None:\n",
        "        Net = globals()[args.model]\n",
        "        gen = Net(args.kernel, args.num_filters)\n",
        "\n",
        "    # LOSS FUNCTION\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(gen.parameters(), lr=args.learn_rate)\n",
        "\n",
        "    # DATA\n",
        "    print(\"Loading data...\")\n",
        "    (x_train, y_train), (x_test, y_test) = load_cifar10()\n",
        "\n",
        "    print(\"Transforming data...\")\n",
        "    train_rgb, train_grey = process(x_train, y_train, downsize_input=args.downsize_input)\n",
        "    test_rgb, test_grey = process(x_test, y_test, downsize_input=args.downsize_input)\n",
        "\n",
        "    # Create the outputs folder if not created already\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    print(\"Beginning training ...\")\n",
        "    if args.gpu:\n",
        "        gen.cuda()\n",
        "    start = time.time()\n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    valid_accs = []\n",
        "    for epoch in range(args.epochs):\n",
        "        # Train the Model\n",
        "        gen.train()  # Change model to 'train' mode\n",
        "        losses = []\n",
        "        for i, (xs, ys) in enumerate(get_batch(train_grey, train_rgb, args.batch_size)):\n",
        "            images, labels = get_torch_vars(xs, ys, args.gpu)\n",
        "            # Forward + Backward + Optimize\n",
        "            optimizer.zero_grad()\n",
        "            outputs = gen(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.data.item())\n",
        "\n",
        "        print(epoch, loss.cpu().detach())\n",
        "        if args.plot:\n",
        "          visual(images, labels, outputs, args.gpu, 1)\n",
        "\n",
        "    return gen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppqKboqz-zX-"
      },
      "source": [
        "Training visualization code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbL_Vlao-yau"
      },
      "outputs": [],
      "source": [
        "# visualize 5 train/test images\n",
        "def visual(img_grey, img_real, img_fake, gpu = 0, flag_torch = 0):\n",
        "\n",
        "  if gpu:\n",
        "    img_grey = img_grey.cpu().detach()\n",
        "    img_real = img_real.cpu().detach()\n",
        "    img_fake = img_fake.cpu().detach()\n",
        "\n",
        "  if flag_torch:\n",
        "    img_grey = img_grey.numpy()\n",
        "    img_real = img_real.numpy()\n",
        "    img_fake = img_fake.numpy()\n",
        "\n",
        "  if flag_torch == 2:\n",
        "    img_real = np.transpose(img_real[:, :, :, :, :], [0, 4, 2, 3, 1]).squeeze()\n",
        "    img_fake = np.transpose(img_fake[:, :, :, :, :], [0, 4, 2, 3, 1]).squeeze()\n",
        "\n",
        "  #correct image structure\n",
        "  img_grey = np.transpose(img_grey[:5, :, :, :], [0, 2, 3, 1]).squeeze()\n",
        "  img_real = np.transpose(img_real[:5, :, :, :], [0, 2, 3, 1])\n",
        "  img_fake = np.transpose(img_fake[:5, :, :, :], [0, 2, 3, 1])\n",
        "\n",
        "  for i in range(5):\n",
        "      ax = plt.subplot(3, 5, i + 1)\n",
        "      ax.imshow(img_grey[i], cmap='gray')\n",
        "      ax.axis(\"off\")\n",
        "      ax = plt.subplot(3, 5, i + 1 + 5)\n",
        "      ax.imshow(img_real[i])\n",
        "      ax.axis(\"off\")\n",
        "      ax = plt.subplot(3, 5, i + 1 + 10)\n",
        "      ax.imshow(img_fake[i])\n",
        "      ax.axis(\"off\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTZWiuxMjQTB"
      },
      "source": [
        "Main training loop for regression CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZHc_eStGAQz"
      },
      "outputs": [],
      "source": [
        "#Main training loop for CNN\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"gpu\": True,\n",
        "    \"valid\": False,\n",
        "    \"checkpoint\": \"\",\n",
        "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
        "    \"model\": \"RegressionCNN\",\n",
        "    \"kernel\": 3,\n",
        "    \"num_filters\": 32,\n",
        "    'learn_rate':0.001,\n",
        "    \"batch_size\": 100,\n",
        "    \"epochs\": 25,\n",
        "    \"seed\": 0,\n",
        "    \"plot\": True,\n",
        "    \"experiment_name\": \"colourization_cnn\",\n",
        "    \"visualize\": False,\n",
        "    \"downsize_input\": False,\n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "cnn = train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KCy7KexsLgl"
      },
      "source": [
        "### Part (i) [2pt EXPLORATORY]\n",
        "Describe the model RegressionCNN. How many convolution layers does it have? What are the filter sizes and number of filters at each layer?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "H_nGLJ4B4rYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXTM_iITgED9"
      },
      "source": [
        "### Part (ii) [2pt EXPLORATORY]\n",
        "Run the regression training code (should run without errors). This will generate some images. How many epochs are we training the CNN model in the given setting?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6qrg8iPp552O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "s_gByotg5tl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5hSmTLxgEWG"
      },
      "source": [
        "### Part (iii) [1pt RESULTS]\n",
        "Re-train a couple of new models using a different number of training epochs. You may train each new models in a new code cell by copying and modifying the code from the last notebook cell. Comment on how the results (output images, training loss) change as we increase or decrease the number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3UUyIcgd55cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "0_24GBrs5uNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1DCIkJq0rMo"
      },
      "source": [
        "### Part (iv) [1pt MODEL]\n",
        "\n",
        "Modify your neural network to include additional convolutional layers. Specifically, explore what happens when you add an additional convolutional layer with the same number of filters as the previous layer and the same kernel size. Add one layer to the encoder and one to the decodor. Ensure that the appropriate parameters are modified (e.g. input and output channels, padding) to accommodate this change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdoXIKRk0rMp"
      },
      "outputs": [],
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb248qFXhq1m"
      },
      "source": [
        "## Part 3 Skip Connections [8 pt]\n",
        "A skip connection in a neural network is a connection which skips one or more layer and connects to a later layer. In this section we will be incorporating skip connections to improve on our model architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "babFMNf_h9UM"
      },
      "source": [
        "### Part (i) [3pt MODEL]\n",
        "Add a skip connection from the first layer to the last, second layer to the second last, etc.\n",
        "That is, the final convolution should have both the output of the previous layer and the initial greyscale input as input. This type of skip-connection architecture and is called a \"UNet\". Following the CNN class that you have completed, complete the __init__ and forward methods of the UNet class.\n",
        "Hint: You will need to use the function torch.cat to add skip connections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsipREOi1r7F"
      },
      "outputs": [],
      "source": [
        "#complete the code\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, kernel, num_filters, num_colours=3, num_in_channels=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Useful parameters\n",
        "        stride = 2\n",
        "        padding = kernel // 2\n",
        "        output_padding = 1\n",
        "\n",
        "        ############### YOUR CODE GOES HERE ###############\n",
        "        ###################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        ############### YOUR CODE GOES HERE ###############\n",
        "        ###################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VzFtk3hh9UN"
      },
      "source": [
        "### Part (ii) [2pt RESULTS]\n",
        "Train the \"UNet\" model for the same amount of epochs as the previous CNN and plot the training curve using a batch size of 100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-vGs7qHndmY"
      },
      "outputs": [],
      "source": [
        "# Main training loop for UNet\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"gpu\": True,\n",
        "    \"valid\": False,\n",
        "    \"checkpoint\": \"\",\n",
        "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
        "    \"model\": \"UNet\",\n",
        "    \"kernel\": 3,\n",
        "    \"num_filters\": 32,\n",
        "    'learn_rate':0.001,\n",
        "    \"batch_size\": 100,\n",
        "    \"epochs\": 25,\n",
        "    \"seed\": 0,\n",
        "    \"plot\": True,\n",
        "    \"experiment_name\": \"colourization_cnn\",\n",
        "    \"visualize\": False,\n",
        "    \"downsize_input\": False,\n",
        "}\n",
        "args.update(args_dict)\n",
        "cnn = train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does the result compare to the previous model? Did skip connections improve the loss? Did the skip connections improve the output qualitatively? How? Give at least two reasons why skip connections might improve the performance of our CNN models."
      ],
      "metadata": {
        "id": "1PWcZWk5TBqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "Y5B6i_U56aAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6fl2PcliJTd"
      },
      "source": [
        "### Part (iii) [2pt RESULTS]\n",
        "Re-train a few more \"UNet\" models using different mini batch sizes with a fixed number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWF83JjpiJTd"
      },
      "outputs": [],
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Describe the effect of batch sizes on the loss and the final image output."
      ],
      "metadata": {
        "id": "8FyGDuy2TKxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "46LC0Ecq6hV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCjRpOlEuuJj"
      },
      "source": [
        "# PART B - Conditional GAN\n",
        "\n",
        "In this second half of the assignment we will construct a conditional generative adversarial network for our image colourization task. This second half of the assignment should be started after the lecture on generative adversarial networks (GANs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNgohSYXiWnD"
      },
      "source": [
        "## Part 1 Conditional GAN [10 Pt]\n",
        "\n",
        "To start we will be modifying the previous sample code to construct and train a conditional GAN. Later you will have the opportunity to explore other modifications that can be made to our architecture to achieve the best results on image colourization model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-TzCQBe_6Uk"
      },
      "source": [
        "### Part (i) [2pt MODEL]\n",
        "Modify the provided training code to implement a generator. Then test to verify it works on the desired input\n",
        "\n",
        "Hint: you can reuse your earlier autoencoder models here to act as a generator, if you do so, the architecture will be more like the pix2pix architecture (see https://phillipi.github.io/pix2pix/ for more details)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRB4eJQIFFAD"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, kernel, num_filters, num_colours=3, num_in_channels=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Useful parameters\n",
        "        stride = 2\n",
        "        padding = kernel // 2\n",
        "        output_padding = 1\n",
        "\n",
        "        ############### YOUR CODE GOES HERE ###############\n",
        "        ###################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        ############### YOUR CODE GOES HERE ###############\n",
        "        ###################################################\n",
        "\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write code to test the generator is working as intended."
      ],
      "metadata": {
        "id": "kVJKZVo56vHe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TpgRI8uYieh"
      },
      "outputs": [],
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o4lkOgF_3A2"
      },
      "source": [
        "### Part (ii) [2pt MODEL]\n",
        "Modify the provided training code to implement a discriminator. Then test to verify it works on the desired input.\n",
        "\n",
        "Hint: You should build a simple CNN model to act as a discriminator for real and fake images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHvxX5jrF0Pw"
      },
      "outputs": [],
      "source": [
        "# discriminator code\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, kernel, num_filters, num_colours=3, num_in_channels=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Useful parameters\n",
        "        stride = 2\n",
        "        padding = kernel // 2\n",
        "        output_padding = 1\n",
        "\n",
        "        ############### YOUR CODE GOES HERE ###############\n",
        "        ###################################################\n",
        "\n",
        "\n",
        "    def forward(self, x, img_greyscale):\n",
        "\n",
        "        ############### YOUR CODE GOES HERE ###############\n",
        "        ###################################################\n",
        "\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write code to test that the discriminator is working as intended."
      ],
      "metadata": {
        "id": "9i_R5LKu6570"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xaw5C6w9Wrlx"
      },
      "outputs": [],
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM8DcScdiWnD"
      },
      "source": [
        "### Part (iii) [2pt MODEL]\n",
        "Modify the provided training code to implement the training needed for a conditional GAN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMPQPSykNTwi"
      },
      "outputs": [],
      "source": [
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "def get_torch_vars(xs, ys, gpu=False):\n",
        "    \"\"\"\n",
        "    Helper function to convert numpy arrays to pytorch tensors.\n",
        "    If GPU is used, move the tensors to GPU.\n",
        "\n",
        "    Args:\n",
        "      xs (float numpy tenosor): greyscale input\n",
        "      ys (int numpy tenosor): categorical labels\n",
        "      gpu (bool): whether to move pytorch tensor to GPU\n",
        "    Returns:\n",
        "      Variable(xs), Variable(ys)\n",
        "    \"\"\"\n",
        "    xs = torch.from_numpy(xs).float()\n",
        "    ys = torch.from_numpy(ys).float() #--> ADDED for cGAN\n",
        "    if gpu:\n",
        "        xs = xs.cuda()\n",
        "        ys = ys.cuda()\n",
        "    return Variable(xs), Variable(ys)\n",
        "\n",
        "def train(args, cnn=None):\n",
        "    # Set the maximum number of threads to prevent crash in Teaching Labs\n",
        "    torch.set_num_threads(5)\n",
        "\n",
        "    # Numpy random seed\n",
        "    npr.seed(args.seed)\n",
        "\n",
        "    # Save directory\n",
        "    save_dir = \"outputs/\" + args.experiment_name\n",
        "\n",
        "    # LOAD THE COLOURS CATEGORIES\n",
        "\n",
        "    # INPUT CHANNEL\n",
        "    num_in_channels = 1 if not args.downsize_input else 3\n",
        "    # LOAD THE MODEL\n",
        "    if cnn is None:\n",
        "        Net = globals()[args.model]\n",
        "        cnn =\n",
        "        discriminator =\n",
        "\n",
        "\n",
        "    # LOSS FUNCTION\n",
        "\n",
        "    criterion =\n",
        "    g_optimizer =\n",
        "    d_optimizer =\n",
        "\n",
        "    # DATA\n",
        "    print(\"Loading data...\")\n",
        "    (x_train, y_train), (x_test, y_test) = load_cifar10()\n",
        "\n",
        "    print(\"Transforming data...\")\n",
        "    train_rgb, train_grey = process(x_train, y_train, downsize_input=args.downsize_input)\n",
        "    test_rgb, test_grey = process(x_test, y_test, downsize_input=args.downsize_input)\n",
        "\n",
        "    # Create the outputs folder if not created already\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    print(\"Beginning training ...\")\n",
        "    if args.gpu:\n",
        "        cnn.cuda()\n",
        "        discriminator.cuda()\n",
        "    start = time.time()\n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    valid_accs = []\n",
        "    for epoch in range(args.epochs):\n",
        "        # Train the Model\n",
        "        cnn.train()\n",
        "        discriminator.train()\n",
        "        losses = []\n",
        "\n",
        "        for i, (xs, ys) in enumerate(get_batch(train_grey, train_rgb, args.batch_size)):\n",
        "            images, labels = get_torch_vars(xs, ys, args.gpu)\n",
        "\n",
        "            #--->ADDED 5\n",
        "            img_grey = images\n",
        "            img_real = labels\n",
        "            batch_size = args.batch_size\n",
        "\n",
        "            #discriminator training\n",
        "            d_optimizer.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            d_loss.backward()\n",
        "            d_optimizer.step()\n",
        "\n",
        "            # generator training\n",
        "            g_optimizer.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "\n",
        "        # print and visualize\n",
        "        print(epoch, g_loss.cpu().detach(), d_loss.cpu().detach())\n",
        "        visual(images, labels, outputs, args.gpu, 1)\n",
        "\n",
        "    return cnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KnN4Bc8iv_Y"
      },
      "source": [
        "### Part (iv) [2pt RESULTS]\n",
        "Train a conditional GAN for image colourization and comment on how the results compare to the autoencoder architectures from part 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efwUBO_Yiv_Y"
      },
      "outputs": [],
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"gpu\": True,\n",
        "    \"valid\": False,\n",
        "    \"checkpoint\": \"\",\n",
        "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
        "    \"model\": \"Generator\",\n",
        "    \"kernel\": 3,\n",
        "    \"num_filters\": 16,\n",
        "    'learn_rate':0.001,\n",
        "    \"batch_size\": 50,\n",
        "    \"epochs\": 100,\n",
        "    \"seed\": 0,\n",
        "    \"plot\": False,\n",
        "    \"experiment_name\": \"colourization_cnn\",\n",
        "    \"visualize\": False,\n",
        "    \"downsize_input\": False,\n",
        "}\n",
        "args.update(args_dict)\n",
        "cnn = train(args)\n",
        "\n",
        "#batch size of 50 with 100 epochs seamed to work"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "XNPFFIZgXhAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (v) [2pt DISCUSSION]\n",
        "\n",
        "There are two ways to implement the conditional GAN. The first is a standard conditional GAN for which the generator is a decoder with an input that is random noise with a conditional. The second is the pix2pix architecture where the generator is a UNet architecture with a greyscale image input.\n",
        "\n",
        "Briefly describe the pros and cons of the two different approaches in regards to the image colourization task.\n",
        "\n",
        "Limit your answer to no more than 100 words."
      ],
      "metadata": {
        "id": "IZYvaepWbBDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "aX1n_qT4bBUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W04U6MhOjezG"
      },
      "source": [
        "## Part 2 New Data [5 Pt]\n",
        "\n",
        "### Part (i) [3pt RESULTS]\n",
        "\n",
        "Retrieve sample pictures from online and demonstrate how well your best model performs. Provide all your code, including any image processing that you need to perform, and provide outputs on all the greyscale images you tested. Your outputs should show the greyscale images, ground truth, and model generated image."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h_wYmbcI7ima"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (ii) [2pt DISCUSSION]\n",
        "\n",
        "Summarize the qualitative performance of your best model on image colourization of new data and provide some reasons why it performs the way it does. Are there certain types of images on which your model works better than on others? Explain.\n",
        "\n"
      ],
      "metadata": {
        "id": "AwXjXf3gO3u1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "HazQG5YvO7eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jda8pXEQ4HKa"
      },
      "source": [
        "## Part 3 Exploration [6 Pt]\n",
        "\n",
        "At this point we have trained a few different generative models for our image colourization task with varying results. What makes this work exciting is that there many other approaches we could take. In this part of the assignment you are asked to consider some modifications you could make to improve the results and provide some comments on why those approaches could be beneficial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvHWPRKv0rNK"
      },
      "source": [
        "### Part (i) [2pt DISCUSSION]\n",
        "\n",
        "We've seen several times in this course how pretrained models could be used to improve performance on classification tasks. Do you think they could be beneficial in improving our results on the image colourization tasks? If so, would it be helpful to implement them in the discriminative and/or generative networks? Why or why not?\n",
        "\n",
        "Limit your answers to no more than 100 words."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "4wN0ln5E7f0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (ii) [2pt DISCUSSION]\n",
        "The CIFAR10 images are 32x32 pixels in resolution. Do you believe that using similar images with a resolution of 250x250 would lead to better results. Why or why not?\n",
        "\n",
        "Limit your answers to no more than 100 words."
      ],
      "metadata": {
        "id": "2Vys9KXtMAHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "Si2QAzSx7gxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (iii) [2pt DISCUSSION]\n",
        "\n",
        "A colour space is a choice of mapping of colours into three-dimensional coordinates. Some colours could be close together in one colour space, but further apart in others. The RGB colour space is probably the most familiar to you, the model used in in our regression colourization example computes squared error in RGB colour space. But, most state of the art colourization models\n",
        "do not use RGB colour space. How could using the RGB colour space be problematic? Your answer should relate how human perception of colour is different than the squared distance. You may use the Wikipedia article on colour space to help you answer the question.\n",
        "\n",
        "Limit your answers to no more than 100 words."
      ],
      "metadata": {
        "id": "k71ooYz_WB_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "rAmJyDlMWF0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART C (Optional) - Bonus Challenge!\n",
        "\n",
        "This is an optional exercise for those that finish the assignment early and would like to perform a deeper exploration of the concepts introduced in the assignment.\n",
        "\n",
        "In Part A we constructed an autoencoder, specifically a UNet architecture with skip connections to learn to colourize images. In Part B completed the same task using conditional GANs. For this bonus challenge we will explore several other techniques that could be used to improve the model's performance on image colourization. A great tutorial on some of these different approaches can be found in a <a href=\"https://towardsdatascience.com/colorizing-black-white-images-with-u-net-and-conditional-gan-a-tutorial-81b2df111cd8\">blog post by Moein Shariatnia</a>. It is highly recommended that you read the article before completing the following tasks:\n",
        "\n",
        "Tasks:\n",
        "\n",
        "1. Modify your conditional GAN architecture to incorporate a pretrained ResNet model to speed up and improve the generative abilities for image colourization.\n",
        "2. Modify your conditional GAN architecture to incorporate the patch discriminator trained on local regions in the images.\n",
        "3. Modify your conditional GAN architecture to use higher resolution images of upto 256 x 256 resolution and compare the performance. Summarize how performance changes with increasing resolution of images. Hint: for this task you will need to use another dataset, perhaps COCO, or ImageNet.\n",
        "4. Modify your conditional GAN architecture to incorporate lab colour space image data representation instead of RGB data. Compare the performance of your model using RGB vs lab colour space data.\n",
        "\n",
        "Bonus marks will be provided based on the number of tasks completed and how well they are completed. Summarize below your results and anything intersting you learned from the steps that you completed. Bonus marks cannot be accumulated beyond a maximum assignment grade."
      ],
      "metadata": {
        "id": "W4-9G8bB1WK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hgILVilnggKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "xvZVSNFw1Zj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYwI4RmFS2RB"
      },
      "source": [
        "### Saving to HTML\n",
        "Detailed instructions for saving to HTML can be found <a href=\"https://stackoverflow.com/questions/53460051/convert-ipynb-notebook-to-html-in-google-colab/64487858#64487858\">here</a>. Provided below are a summary of the instructions:\n",
        "\n",
        "(1) download your ipynb file by clicking on File->Download.ipynb\n",
        "\n",
        "(2) reupload your file to the temporary Google Colab storage (you can access the temporary storage from the tab to the left)\n",
        "\n",
        "(3) run the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TrsqdNgS5ex"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html /content/A3.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuXhlFlPTY7F"
      },
      "source": [
        "(4) the html file will be available for download in the temporary Google Colab storage\n",
        "\n",
        "(5) review the html file and make sure all the results are visible before submitting your assignment to Quercus"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Grading Rubric\n",
        "The grading of the assignment will be based on the following categories:\n",
        "\n",
        "(1) **10 Pt - EXPLORATORY QUESTIONS** These are basic questions that in most cases can be answered without requiring a fully working and trained neural network model. For example, data loading, processing and visualization, summary statistics, data exploration, model and training setup, etc.\n",
        "\n",
        "(2) **10 Pt - MODEL** Student has successfully implemented all the required neural network models and has demonstrated successful training of the model without any errors.\n",
        "\n",
        "(3) **10 Pt - RESULT** Students are evaluated based on the results achieved in comparison to the expected results of the assignment.\n",
        "\n",
        "(4) **10 Pt - DISCUSSION QUESTIONS** Student demonstrated understanding beyond the basic exploratory questions, can answer some of the more challenging questions, and provide arguments for their model selection decisions.\n",
        "\n",
        "(5) **10 Pt - COMMUNICATION** Student has provided a quality submission that is easy to read without too many unnecessary output statements that distract the reading of the document. The code has been well commented and all the answers are communicated clearly and concisely.\n",
        "\n",
        "(6) **10 Pt - BONUS** Student has completed the assignment and has taken on the challenging bonus tasks listed in PART C. The student has demonstrated a good understanding of all aspects of the assignment and has exceeded expectations for the assignment.\n",
        "\n",
        "\n",
        "\n",
        "**TOTAL GRADE = _____ of 50 Pts**"
      ],
      "metadata": {
        "id": "HKmW9v0413iX"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}